#!/usr/bin/env node

/**
 * OllamaæœåŠ¡æ£€æŸ¥å’Œç®¡ç†è„šæœ¬
 * æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€ï¼Œç®¡ç†æ¨¡å‹ï¼Œæä¾›æ•…éšœæ’é™¤
 */

import { execSync } from "child_process"

interface OllamaModel {
  name: string
  size: string
  modified: string
}

interface OllamaServiceInfo {
  isRunning: boolean
  version?: string
  models: OllamaModel[]
  url: string
}

class OllamaServiceChecker {
  private defaultUrl = "http://localhost:11434"

  // æ£€æŸ¥Ollamaæ˜¯å¦å·²å®‰è£…
  private checkOllamaInstallation(): boolean {
    try {
      const version = execSync("ollama --version", { encoding: "utf-8", stdio: "pipe" })
      console.log(`âœ… Ollamaå·²å®‰è£…: ${version.trim()}`)
      return true
    } catch {
      console.log("âŒ Ollamaæœªå®‰è£…")
      return false
    }
  }

  // æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€
  private async checkOllamaService(url: string = this.defaultUrl): Promise<boolean> {
    try {
      const response = await fetch(`${url}/api/tags`, {
        signal: AbortSignal.timeout(3000),
      })
      return response.ok
    } catch {
      return false
    }
  }

  // è·å–Ollamaç‰ˆæœ¬ä¿¡æ¯
  private async getOllamaVersion(url: string): Promise<string | null> {
    try {
      const response = await fetch(`${url}/api/version`, {
        signal: AbortSignal.timeout(3000),
      })
      if (response.ok) {
        const data = await response.json()
        return data.version
      }
    } catch {
      // å¿½ç•¥é”™è¯¯
    }
    return null
  }

  // è·å–å·²å®‰è£…çš„æ¨¡å‹åˆ—è¡¨
  private async getInstalledModels(url: string): Promise<OllamaModel[]> {
    try {
      const response = await fetch(`${url}/api/tags`, {
        signal: AbortSignal.timeout(5000),
      })
      if (response.ok) {
        const data = await response.json()
        return data.models || []
      }
    } catch {
      // å¿½ç•¥é”™è¯¯
    }
    return []
  }

  // æµ‹è¯•æ¨¡å‹æ¨ç†
  private async testModelInference(url: string, model: string): Promise<boolean> {
    try {
      const response = await fetch(`${url}/api/generate`, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          model,
          prompt: "Hello",
          stream: false,
        }),
        signal: AbortSignal.timeout(10000),
      })
      return response.ok
    } catch {
      return false
    }
  }

  // è·å–æ¨èæ¨¡å‹åˆ—è¡¨
  private getRecommendedModels(): Array<{ name: string; description: string; size: string }> {
    return [
      {
        name: "llama3.2",
        description: "é€šç”¨å¯¹è¯æ¨¡å‹ï¼Œé€‚åˆæ—¥å¸¸äº¤äº’",
        size: "2.0GB",
      },
      {
        name: "codellama",
        description: "ä¸“ä¸šä»£ç ç”Ÿæˆæ¨¡å‹",
        size: "3.8GB",
      },
      {
        name: "qwen2.5",
        description: "ä¸­æ–‡ä¼˜åŒ–æ¨¡å‹ï¼Œæ”¯æŒä¸­æ–‡å¯¹è¯",
        size: "4.4GB",
      },
      {
        name: "llama3.2:1b",
        description: "è½»é‡çº§æ¨¡å‹ï¼Œå¿«é€Ÿå“åº”",
        size: "1.3GB",
      },
      {
        name: "deepseek-coder",
        description: "æ·±åº¦ä»£ç ç†è§£æ¨¡å‹",
        size: "6.9GB",
      },
    ]
  }

  // æä¾›å®‰è£…æŒ‡å¯¼
  private provideInstallationGuide(): void {
    console.log("\nğŸ“– Ollamaå®‰è£…æŒ‡å—:")
    console.log("=".repeat(40))

    console.log("\nğŸ–¥ï¸ å„å¹³å°å®‰è£…æ–¹æ³•:")
    console.log("  macOS:")
    console.log("    curl -fsSL https://ollama.ai/install.sh | sh")
    console.log("    æˆ–ä¸‹è½½: https://ollama.ai/download/Ollama-darwin.zip")

    console.log("\n  Linux:")
    console.log("    curl -fsSL https://ollama.ai/install.sh | sh")

    console.log("\n  Windows:")
    console.log("    ä¸‹è½½å®‰è£…åŒ…: https://ollama.ai/download/OllamaSetup.exe")

    console.log("\n  Docker:")
    console.log("    docker run -d -p 11434:11434 --name ollama ollama/ollama")

    console.log("\nğŸš€ å®‰è£…åå¯åŠ¨æœåŠ¡:")
    console.log("    ollama serve")

    console.log("\nğŸ“¦ ä¸‹è½½æ¨èæ¨¡å‹:")
    const recommended = this.getRecommendedModels()
    recommended.forEach((model) => {
      console.log(`    ollama pull ${model.name}  # ${model.description} (${model.size})`)
    })
  }

  // æä¾›æ•…éšœæ’é™¤æŒ‡å¯¼
  private provideTroubleshootingGuide(): void {
    console.log("\nğŸ”§ æ•…éšœæ’é™¤æŒ‡å—:")
    console.log("=".repeat(40))

    console.log("\nâ“ å¸¸è§é—®é¢˜:")

    console.log("\n1. æœåŠ¡æ— æ³•å¯åŠ¨:")
    console.log("   - æ£€æŸ¥ç«¯å£11434æ˜¯å¦è¢«å ç”¨: lsof -i :11434")
    console.log("   - å°è¯•æŒ‡å®šä¸åŒç«¯å£: OLLAMA_HOST=0.0.0.0:11435 ollama serve")
    console.log("   - æ£€æŸ¥é˜²ç«å¢™è®¾ç½®")

    console.log("\n2. æ¨¡å‹ä¸‹è½½å¤±è´¥:")
    console.log("   - æ£€æŸ¥ç½‘ç»œè¿æ¥")
    console.log("   - å°è¯•ä½¿ç”¨ä»£ç†: HTTPS_PROXY=http://proxy:port ollama pull model")
    console.log("   - æ£€æŸ¥ç£ç›˜ç©ºé—´")

    console.log("\n3. æ¨ç†é€Ÿåº¦æ…¢:")
    console.log("   - ä½¿ç”¨GPUåŠ é€Ÿ (éœ€è¦NVIDIA GPU)")
    console.log("   - é€‰æ‹©æ›´å°çš„æ¨¡å‹")
    console.log("   - å¢åŠ ç³»ç»Ÿå†…å­˜")

    console.log("\n4. å†…å­˜ä¸è¶³:")
    console.log("   - å…³é—­å…¶ä»–åº”ç”¨ç¨‹åº")
    console.log("   - ä½¿ç”¨æ›´å°çš„æ¨¡å‹ (å¦‚ llama3.2:1b)")
    console.log("   - è°ƒæ•´æ¨¡å‹å‚æ•°")

    console.log("\nğŸ” è¯Šæ–­å‘½ä»¤:")
    console.log("   ollama list                    # æŸ¥çœ‹å·²å®‰è£…æ¨¡å‹")
    console.log("   ollama show model-name         # æŸ¥çœ‹æ¨¡å‹è¯¦æƒ…")
    console.log("   ollama ps                      # æŸ¥çœ‹è¿è¡Œä¸­çš„æ¨¡å‹")
    console.log("   curl http://localhost:11434/api/tags  # æµ‹è¯•API")
  }

  // ä¸»è¦æ£€æŸ¥æµç¨‹
  public async check(): Promise<void> {
    console.log("ğŸ” OllamaæœåŠ¡æ£€æŸ¥")
    console.log("=".repeat(50))

    // æ£€æŸ¥å®‰è£…çŠ¶æ€
    const isInstalled = this.checkOllamaInstallation()
    if (!isInstalled) {
      this.provideInstallationGuide()
      return
    }

    // æ£€æŸ¥æœåŠ¡çŠ¶æ€
    console.log("\nğŸ”— æ£€æŸ¥æœåŠ¡çŠ¶æ€...")
    const isRunning = await this.checkOllamaService()

    if (!isRunning) {
      console.log("âŒ OllamaæœåŠ¡æœªè¿è¡Œ")
      console.log("\nğŸš€ å¯åŠ¨æœåŠ¡:")
      console.log("   ollama serve")
      console.log("   æˆ–åå°è¿è¡Œ: nohup ollama serve > ollama.log 2>&1 &")

      this.provideTroubleshootingGuide()
      return
    }

    console.log("âœ… OllamaæœåŠ¡æ­£åœ¨è¿è¡Œ")

    // è·å–ç‰ˆæœ¬ä¿¡æ¯
    const version = await this.getOllamaVersion(this.defaultUrl)
    if (version) {
      console.log(`ğŸ“¦ æœåŠ¡ç‰ˆæœ¬: ${version}`)
    }

    // è·å–æ¨¡å‹åˆ—è¡¨
    console.log("\nğŸ“‹ å·²å®‰è£…æ¨¡å‹:")
    const models = await this.getInstalledModels(this.defaultUrl)

    if (models.length === 0) {
      console.log("   âš ï¸ æœªå®‰è£…ä»»ä½•æ¨¡å‹")
      console.log("\nğŸ“¦ æ¨èå®‰è£…ä»¥ä¸‹æ¨¡å‹:")
      const recommended = this.getRecommendedModels()
      recommended.slice(0, 3).forEach((model) => {
        console.log(`   ollama pull ${model.name}  # ${model.description}`)
      })
    } else {
      for (const model of models) {
        console.log(`   âœ… ${model.name} (${model.size})`)

        // æµ‹è¯•æ¨¡å‹æ¨ç†
        console.log(`      ğŸ§ª æµ‹è¯•æ¨ç†...`)
        const canInfer = await this.testModelInference(this.defaultUrl, model.name)
        if (canInfer) {
          console.log(`      âœ… æ¨ç†æ­£å¸¸`)
        } else {
          console.log(`      âŒ æ¨ç†å¤±è´¥`)
        }
      }
    }

    // æ€§èƒ½å»ºè®®
    console.log("\nâš¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®:")
    if (models.length > 3) {
      console.log("   - è€ƒè™‘åˆ é™¤ä¸å¸¸ç”¨çš„æ¨¡å‹ä»¥èŠ‚çœç©ºé—´")
    }
    console.log("   - ä½¿ç”¨GPUåŠ é€Ÿå¯æ˜¾è‘—æå‡æ€§èƒ½")
    console.log("   - æ ¹æ®ç”¨é€”é€‰æ‹©åˆé€‚å¤§å°çš„æ¨¡å‹")

    // ç¯å¢ƒå˜é‡å»ºè®®
    console.log("\nğŸ”§ ç¯å¢ƒå˜é‡é…ç½®:")
    console.log(`   NEXT_PUBLIC_OLLAMA_URL=${this.defaultUrl}`)

    console.log("\nğŸ‰ OllamaæœåŠ¡æ£€æŸ¥å®Œæˆ!")
    console.log("   æœåŠ¡åœ°å€:", this.defaultUrl)
    console.log("   å¯ç”¨æ¨¡å‹:", models.length)
    console.log("   çŠ¶æ€: æ­£å¸¸è¿è¡Œ")
  }
}

// å¦‚æœç›´æ¥è¿è¡Œæ­¤è„šæœ¬
if (require.main === module) {
  const checker = new OllamaServiceChecker()
  checker.check()
}

export { OllamaServiceChecker }
