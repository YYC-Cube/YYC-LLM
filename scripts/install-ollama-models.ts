#!/usr/bin/env node

/**
 * Ollamaæ¨¡å‹å®‰è£…è„šæœ¬
 * è‡ªåŠ¨å®‰è£…æ¨èçš„AIæ¨¡å‹
 */

import { execSync } from "child_process"

interface ModelInfo {
  name: string
  description: string
  size: string
  category: string
  priority: number
  recommended: boolean
}

class OllamaModelInstaller {
  private models: ModelInfo[] = [
    {
      name: "llama3.2:1b",
      description: "è½»é‡çº§é€šç”¨æ¨¡å‹ï¼Œå¿«é€Ÿå“åº”",
      size: "1.3GB",
      category: "general",
      priority: 1,
      recommended: true,
    },
    {
      name: "qwen2.5:1.5b",
      description: "ä¸­æ–‡ä¼˜åŒ–è½»é‡æ¨¡å‹",
      size: "1.5GB",
      category: "chinese",
      priority: 2,
      recommended: true,
    },
    {
      name: "codellama:7b",
      description: "ä»£ç ç”Ÿæˆä¸“ç”¨æ¨¡å‹",
      size: "3.8GB",
      category: "coding",
      priority: 3,
      recommended: true,
    },
    {
      name: "llama3.2:3b",
      description: "å¹³è¡¡æ€§èƒ½å’Œè´¨é‡çš„é€šç”¨æ¨¡å‹",
      size: "2.0GB",
      category: "general",
      priority: 4,
      recommended: false,
    },
    {
      name: "qwen2.5:7b",
      description: "é«˜è´¨é‡ä¸­æ–‡å¯¹è¯æ¨¡å‹",
      size: "4.4GB",
      category: "chinese",
      priority: 5,
      recommended: false,
    },
    {
      name: "deepseek-coder:6.7b",
      description: "æ·±åº¦ä»£ç ç†è§£æ¨¡å‹",
      size: "6.9GB",
      category: "coding",
      priority: 6,
      recommended: false,
    },
    {
      name: "llama3.1:8b",
      description: "é«˜æ€§èƒ½é€šç”¨å¤§æ¨¡å‹",
      size: "4.7GB",
      category: "general",
      priority: 7,
      recommended: false,
    },
  ]

  // æ£€æŸ¥Ollamaæ˜¯å¦å¯ç”¨
  private checkOllamaAvailable(): boolean {
    try {
      execSync("ollama --version", { stdio: "pipe" })
      return true
    } catch {
      return false
    }
  }

  // æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦è¿è¡Œ
  private async checkOllamaService(): Promise<boolean> {
    try {
      const response = await fetch("http://localhost:11434/api/tags", {
        signal: AbortSignal.timeout(3000),
      })
      return response.ok
    } catch {
      return false
    }
  }

  // è·å–å·²å®‰è£…çš„æ¨¡å‹
  private getInstalledModels(): string[] {
    try {
      const output = execSync("ollama list", { encoding: "utf-8", stdio: "pipe" })
      const lines = output.split("\n").slice(1) // è·³è¿‡æ ‡é¢˜è¡Œ
      return lines
        .filter((line) => line.trim())
        .map((line) => line.split(/\s+/)[0])
        .filter((name) => name && name !== "NAME")
    } catch {
      return []
    }
  }

  // æ£€æŸ¥ç£ç›˜ç©ºé—´
  private checkDiskSpace(): number {
    try {
      const output = execSync("df -h .", { encoding: "utf-8", stdio: "pipe" })
      const lines = output.split("\n")
      if (lines.length > 1) {
        const parts = lines[1].split(/\s+/)
        const available = parts[3]
        // ç®€å•è§£æå¯ç”¨ç©ºé—´ (GB)
        const match = available.match(/(\d+(?:\.\d+)?)([KMGT]?)/)
        if (match) {
          const [, size, unit] = match
          const sizeNum = Number.parseFloat(size)
          switch (unit) {
            case "T":
              return sizeNum * 1024
            case "G":
              return sizeNum
            case "M":
              return sizeNum / 1024
            case "K":
              return sizeNum / (1024 * 1024)
            default:
              return sizeNum / (1024 * 1024 * 1024)
          }
        }
      }
    } catch {
      // å¿½ç•¥é”™è¯¯ï¼Œè¿”å›é»˜è®¤å€¼
    }
    return 10 // é»˜è®¤å‡è®¾æœ‰10GBå¯ç”¨ç©ºé—´
  }

  // å®‰è£…å•ä¸ªæ¨¡å‹
  private async installModel(modelName: string): Promise<boolean> {
    console.log(`ğŸ“¦ æ­£åœ¨å®‰è£…æ¨¡å‹: ${modelName}`)

    try {
      // ä½¿ç”¨spawnæ¥å®æ—¶æ˜¾ç¤ºè¿›åº¦
      const process = execSync(`ollama pull ${modelName}`, {
        encoding: "utf-8",
        stdio: "inherit", // å®æ—¶æ˜¾ç¤ºè¾“å‡º
      })

      console.log(`âœ… æ¨¡å‹å®‰è£…æˆåŠŸ: ${modelName}`)
      return true
    } catch (error) {
      console.log(`âŒ æ¨¡å‹å®‰è£…å¤±è´¥: ${modelName}`)
      console.log(`   é”™è¯¯: ${error}`)
      return false
    }
  }

  // æµ‹è¯•æ¨¡å‹
  private async testModel(modelName: string): Promise<boolean> {
    console.log(`ğŸ§ª æµ‹è¯•æ¨¡å‹: ${modelName}`)

    try {
      const response = await fetch("http://localhost:11434/api/generate", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          model: modelName,
          prompt: "Hello, how are you?",
          stream: false,
        }),
        signal: AbortSignal.timeout(30000), // 30ç§’è¶…æ—¶
      })

      if (response.ok) {
        const data = await response.json()
        if (data.response) {
          console.log(`âœ… æ¨¡å‹æµ‹è¯•æˆåŠŸ: ${modelName}`)
          console.log(`   å“åº”: ${data.response.substring(0, 100)}...`)
          return true
        }
      }
    } catch (error) {
      console.log(`âŒ æ¨¡å‹æµ‹è¯•å¤±è´¥: ${modelName}`)
    }

    return false
  }

  // æ¨èæ¨¡å‹é€‰æ‹©
  private recommendModels(availableSpace: number): ModelInfo[] {
    const recommended: ModelInfo[] = []
    let totalSize = 0

    // æŒ‰ä¼˜å…ˆçº§æ’åº
    const sortedModels = [...this.models].sort((a, b) => a.priority - b.priority)

    for (const model of sortedModels) {
      const modelSize = Number.parseFloat(model.size.replace("GB", ""))

      if (totalSize + modelSize <= availableSpace * 0.8) {
        // ä¿ç•™20%ç©ºé—´
        recommended.push(model)
        totalSize += modelSize

        // è‡³å°‘å®‰è£…3ä¸ªæ¨èæ¨¡å‹
        if (recommended.length >= 3 && !model.recommended) {
          break
        }
      }
    }

    return recommended
  }

  // æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
  private displayModelInfo(): void {
    console.log("\nğŸ“‹ å¯ç”¨æ¨¡å‹åˆ—è¡¨:")
    console.log("=".repeat(60))

    const categories = ["general", "chinese", "coding"]
    const categoryNames = {
      general: "ğŸŒŸ é€šç”¨æ¨¡å‹",
      chinese: "ğŸ‡¨ğŸ‡³ ä¸­æ–‡æ¨¡å‹",
      coding: "ğŸ’» ç¼–ç¨‹æ¨¡å‹",
    }

    for (const category of categories) {
      console.log(`\n${categoryNames[category as keyof typeof categoryNames]}:`)

      const categoryModels = this.models.filter((m) => m.category === category)
      for (const model of categoryModels) {
        const badge = model.recommended ? "â­" : "  "
        console.log(`  ${badge} ${model.name.padEnd(20)} ${model.size.padEnd(8)} ${model.description}`)
      }
    }
  }

  // äº¤äº’å¼é€‰æ‹©æ¨¡å‹
  private async selectModelsInteractively(): Promise<string[]> {
    // è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…å¯ä»¥ä½¿ç”¨inquirerç­‰åº“å®ç°äº¤äº’
    console.log("\nğŸ¯ æ¨èå®‰è£…ä»¥ä¸‹æ¨¡å‹:")

    const availableSpace = this.checkDiskSpace()
    const recommended = this.recommendModels(availableSpace)

    console.log(`ğŸ’¾ å¯ç”¨ç£ç›˜ç©ºé—´: ${availableSpace.toFixed(1)}GB`)
    console.log("ğŸ“¦ æ¨èæ¨¡å‹:")

    let totalSize = 0
    for (const model of recommended) {
      const size = Number.parseFloat(model.size.replace("GB", ""))
      totalSize += size
      console.log(`   âœ“ ${model.name} (${model.size}) - ${model.description}`)
    }

    console.log(`ğŸ“Š æ€»è®¡å¤§å°: ${totalSize.toFixed(1)}GB`)

    return recommended.map((m) => m.name)
  }

  // ä¸»è¦å®‰è£…æµç¨‹
  public async install(): Promise<void> {
    console.log("ğŸš€ Ollamaæ¨¡å‹å®‰è£…å™¨")
    console.log("=".repeat(50))

    // æ£€æŸ¥Ollamaå¯ç”¨æ€§
    if (!this.checkOllamaAvailable()) {
      console.log("âŒ Ollamaæœªå®‰è£…æˆ–ä¸å¯ç”¨")
      console.log("   è¯·å…ˆå®‰è£…Ollama: https://ollama.ai/")
      return
    }

    // æ£€æŸ¥æœåŠ¡çŠ¶æ€
    const isServiceRunning = await this.checkOllamaService()
    if (!isServiceRunning) {
      console.log("âŒ OllamaæœåŠ¡æœªè¿è¡Œ")
      console.log("   è¯·å¯åŠ¨æœåŠ¡: ollama serve")
      return
    }

    console.log("âœ… OllamaæœåŠ¡æ­£å¸¸è¿è¡Œ")

    // è·å–å·²å®‰è£…æ¨¡å‹
    const installedModels = this.getInstalledModels()
    console.log(`ğŸ“¦ å·²å®‰è£…æ¨¡å‹: ${installedModels.length}ä¸ª`)

    if (installedModels.length > 0) {
      console.log("   å·²å®‰è£…:")
      installedModels.forEach((model) => console.log(`     - ${model}`))
    }

    // æ˜¾ç¤ºå¯ç”¨æ¨¡å‹
    this.displayModelInfo()

    // é€‰æ‹©è¦å®‰è£…çš„æ¨¡å‹
    const modelsToInstall = await this.selectModelsInteractively()

    // è¿‡æ»¤å·²å®‰è£…çš„æ¨¡å‹
    const newModels = modelsToInstall.filter((model) => !installedModels.includes(model))

    if (newModels.length === 0) {
      console.log("âœ… æ‰€æœ‰æ¨èæ¨¡å‹å·²å®‰è£…")
      return
    }

    console.log(`\nğŸ”„ å‡†å¤‡å®‰è£… ${newModels.length} ä¸ªæ–°æ¨¡å‹...`)

    // å®‰è£…æ¨¡å‹
    const results: Array<{ model: string; success: boolean }> = []

    for (const model of newModels) {
      const success = await this.installModel(model)
      results.push({ model, success })

      if (success) {
        // æµ‹è¯•æ¨¡å‹
        await this.testModel(model)
      }

      console.log("") // ç©ºè¡Œåˆ†éš”
    }

    // å®‰è£…ç»“æœç»Ÿè®¡
    const successful = results.filter((r) => r.success).length
    const failed = results.filter((r) => !r.success).length

    console.log("\nğŸ“Š å®‰è£…ç»“æœ:")
    console.log("=".repeat(30))
    console.log(`âœ… æˆåŠŸ: ${successful}ä¸ª`)
    console.log(`âŒ å¤±è´¥: ${failed}ä¸ª`)

    if (failed > 0) {
      console.log("\nâŒ å®‰è£…å¤±è´¥çš„æ¨¡å‹:")
      results
        .filter((r) => !r.success)
        .forEach((r) => {
          console.log(`   - ${r.model}`)
        })
    }

    // æœ€ç»ˆå»ºè®®
    console.log("\nğŸ‰ æ¨¡å‹å®‰è£…å®Œæˆ!")
    console.log("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
    console.log("   - è½»é‡ä»»åŠ¡ä½¿ç”¨: llama3.2:1b")
    console.log("   - ä¸­æ–‡å¯¹è¯ä½¿ç”¨: qwen2.5:1.5b")
    console.log("   - ä»£ç ç”Ÿæˆä½¿ç”¨: codellama:7b")

    console.log("\nğŸ”§ ç¯å¢ƒå˜é‡é…ç½®:")
    console.log("   NEXT_PUBLIC_OLLAMA_URL=http://localhost:11434")

    console.log("\nğŸš€ ç°åœ¨å¯ä»¥å¯åŠ¨åº”ç”¨:")
    console.log("   npm run dev")
  }
}

// å¦‚æœç›´æ¥è¿è¡Œæ­¤è„šæœ¬
if (require.main === module) {
  const installer = new OllamaModelInstaller()
  installer.install()
}

export { OllamaModelInstaller }
