"use client"

// å¢å¼ºç‰ˆOllamaæœåŠ¡ - ä¼˜åŒ–æ€§èƒ½å’Œç¨³å®šæ€§
export class EnhancedOllamaService {
  private static instance: EnhancedOllamaService
  private baseUrl: string
  private models = new Map<string, EnhancedOllamaModel>()
  private downloadQueue: DownloadTask[] = []
  private activeDownloads = new Map<string, DownloadTask>()
  private maxConcurrentDownloads = 2
  private healthCheckInterval: NodeJS.Timeout | null = null
  private reconnectAttempts = 0
  private maxReconnectAttempts = 5
  private connectionStatus: "connected" | "disconnected" | "reconnecting" = "disconnected"
  private listeners = new Map<string, Set<(payload?: any) => void>>()

  private constructor() {
    this.baseUrl = process.env.NEXT_PUBLIC_OLLAMA_URL || "http://localhost:11434"
    this.initializeService()
  }

  public on(event: string, listener: (payload?: any) => void): void {
    const set = this.listeners.get(event) || new Set()
    set.add(listener)
    this.listeners.set(event, set)
  }

  public once(event: string, listener: (payload?: any) => void): void {
    const wrapper = (payload?: any) => {
      this.off(event, wrapper)
      listener(payload)
    }
    this.on(event, wrapper)
  }

  public off(event: string, listener: (payload?: any) => void): void {
    const set = this.listeners.get(event)
    if (set) {
      set.delete(listener)
      if (set.size === 0) this.listeners.delete(event)
    }
  }

  public removeListener(event: string, listener: (payload?: any) => void): void {
    this.off(event, listener)
  }

  public emit(event: string, payload?: any): void {
    const set = this.listeners.get(event)
    if (set) {
      for (const fn of Array.from(set)) {
        try {
          fn(payload)
        } catch (e) {
          console.error(`äº‹ä»¶ç›‘å¬å™¨æ‰§è¡Œå¤±è´¥: ${event}`, e)
        }
      }
    }
  }

  public static getInstance(): EnhancedOllamaService {
    if (!EnhancedOllamaService.instance) {
      EnhancedOllamaService.instance = new EnhancedOllamaService()
    }
    return EnhancedOllamaService.instance
  }

  // åˆå§‹åŒ–æœåŠ¡
  private async initializeService(): Promise<void> {
    try {
      await this.checkConnection()
      await this.loadModels()
      this.startHealthCheck()
      this.processDownloadQueue()

      console.log("âœ… Enhanced OllamaæœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
      this.emit("service:ready")
    } catch (error) {
      console.error("âŒ Enhanced OllamaæœåŠ¡åˆå§‹åŒ–å¤±è´¥:", error)
      this.emit("service:error", error)
      this.scheduleReconnect()
    }
  }

  // æ£€æŸ¥è¿æ¥çŠ¶æ€
  private async checkConnection(): Promise<boolean> {
    try {
      const controller = new AbortController()
      const timeoutId = setTimeout(() => controller.abort(), 5000)

      const response = await fetch(`${this.baseUrl}/api/tags`, {
        method: "GET",
        signal: controller.signal,
        headers: {
          "Content-Type": "application/json",
        },
      })

      clearTimeout(timeoutId)

      if (response.ok) {
        this.connectionStatus = "connected"
        this.reconnectAttempts = 0
        this.emit("connection:established")
        return true
      } else {
        throw new Error(`HTTP ${response.status}: ${response.statusText}`)
      }
    } catch (error) {
      this.connectionStatus = "disconnected"
      this.emit("connection:lost", error)
      throw error
    }
  }

  // åŠ è½½æ¨¡å‹åˆ—è¡¨
  private async loadModels(): Promise<void> {
    try {
      const response = await this.makeRequest("/api/tags", "GET")
      const data = await response.json()

      if (data.models) {
        // æ¸…ç©ºç°æœ‰æ¨¡å‹åˆ—è¡¨
        this.models.clear()

        // æ·»åŠ å·²å®‰è£…çš„æ¨¡å‹
        for (const model of data.models) {
          const enhancedModel: EnhancedOllamaModel = {
            id: model.name,
            name: this.formatModelName(model.name),
            type: this.inferModelType(model.name),
            provider: "ollama",
            status: "ready",
            size: model.size || 0,
            digest: model.digest,
            modifiedAt: new Date(model.modified_at),
            parameters: this.inferModelParameters(model.name),
            quantization: this.inferModelQuantization(model.name),
            usageStats: {
              totalCalls: 0,
              totalTokens: 0,
              averageLatency: 0,
              lastUsed: null,
              errorCount: 0,
            },
            performance: {
              tokensPerSecond: 0,
              memoryUsage: 0,
              cpuUsage: 0,
            },
            createdAt: new Date(),
          }

          this.models.set(model.name, enhancedModel)
        }

        // æ·»åŠ æ¨èä½†æœªå®‰è£…çš„æ¨¡å‹
        this.addRecommendedModels()

        console.log(`ğŸ“Š å·²åŠ è½½ ${this.models.size} ä¸ªæ¨¡å‹`)
        this.emit("models:loaded", Array.from(this.models.values()))
      }
    } catch (error) {
      console.error("åŠ è½½æ¨¡å‹åˆ—è¡¨å¤±è´¥:", error)
      throw error
    }
  }

  // æ·»åŠ æ¨èæ¨¡å‹
  private addRecommendedModels(): void {
    const recommendedModels = [
      { id: "llama3:8b", name: "Llama 3 8B", type: "chat" as ModelType },
      { id: "codellama:7b", name: "CodeLlama 7B", type: "code" as ModelType },
      { id: "phi3:mini", name: "Phi-3 Mini", type: "chat" as ModelType },
      { id: "qwen2:7b", name: "Qwen2 7B", type: "chat" as ModelType },
      { id: "mistral:7b", name: "Mistral 7B", type: "chat" as ModelType },
      { id: "gemma:7b", name: "Gemma 7B", type: "chat" as ModelType },
    ]

    for (const recommended of recommendedModels) {
      if (!this.models.has(recommended.id)) {
        const enhancedModel: EnhancedOllamaModel = {
          id: recommended.id,
          name: recommended.name,
          type: recommended.type,
          provider: "ollama",
          status: "not_downloaded",
          size: 0,
          digest: "",
          modifiedAt: new Date(),
          parameters: this.inferModelParameters(recommended.id),
          quantization: this.inferModelQuantization(recommended.id),
          usageStats: {
            totalCalls: 0,
            totalTokens: 0,
            averageLatency: 0,
            lastUsed: null,
            errorCount: 0,
          },
          performance: {
            tokensPerSecond: 0,
            memoryUsage: 0,
            cpuUsage: 0,
          },
          createdAt: new Date(),
        }

        this.models.set(recommended.id, enhancedModel)
      }
    }
  }

  // ä¼˜åŒ–çš„HTTPè¯·æ±‚æ–¹æ³•
  private async makeRequest(
    endpoint: string,
    method: "GET" | "POST" | "DELETE" = "GET",
    body?: any,
    timeout = 30000,
  ): Promise<Response> {
    const controller = new AbortController()
    const timeoutId = setTimeout(() => controller.abort(), timeout)

    try {
      const response = await fetch(`${this.baseUrl}${endpoint}`, {
        method,
        headers: {
          "Content-Type": "application/json",
        },
        body: body ? JSON.stringify(body) : undefined,
        signal: controller.signal,
      })

      clearTimeout(timeoutId)

      if (!response.ok) {
        throw new Error(`HTTP ${response.status}: ${response.statusText}`)
      }

      return response
    } catch (error) {
      clearTimeout(timeoutId)

      if (error instanceof Error && error.name === "AbortError") {
        throw new Error("è¯·æ±‚è¶…æ—¶")
      }

      throw error
    }
  }

  // ä¸‹è½½æ¨¡å‹ï¼ˆæ”¯æŒé˜Ÿåˆ—å’Œå¹¶å‘æ§åˆ¶ï¼‰
  public async downloadModel(modelId: string): Promise<DownloadTask> {
    // æ£€æŸ¥æ˜¯å¦å·²åœ¨ä¸‹è½½é˜Ÿåˆ—ä¸­
    const existingTask =
      this.downloadQueue.find((task) => task.modelId === modelId) || this.activeDownloads.get(modelId)

    if (existingTask) {
      return existingTask
    }

    // åˆ›å»ºä¸‹è½½ä»»åŠ¡
    const task: DownloadTask = {
      id: `download_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`,
      modelId,
      type: "download",
      status: "queued",
      progress: 0,
      speed: 0,
      eta: null,
      createdAt: new Date(),
      updatedAt: new Date(),
    }

    // æ·»åŠ åˆ°é˜Ÿåˆ—
    this.downloadQueue.push(task)
    this.emit("download:queued", task)

    // æ›´æ–°æ¨¡å‹çŠ¶æ€
    const model = this.models.get(modelId)
    if (model) {
      model.status = "queued"
      this.models.set(modelId, model)
      this.emit("model:updated", model)
    }

    // å¤„ç†é˜Ÿåˆ—
    this.processDownloadQueue()

    return task
  }

  // å¤„ç†ä¸‹è½½é˜Ÿåˆ—
  private async processDownloadQueue(): Promise<void> {
    // æ£€æŸ¥æ˜¯å¦æœ‰ç©ºé—²çš„ä¸‹è½½æ§½ä½
    while (this.activeDownloads.size < this.maxConcurrentDownloads && this.downloadQueue.length > 0) {
      const task = this.downloadQueue.shift()
      if (task) {
        this.activeDownloads.set(task.modelId, task)
        this.startModelDownload(task)
      }
    }
  }

  // å¼€å§‹æ¨¡å‹ä¸‹è½½
  private async startModelDownload(task: DownloadTask): Promise<void> {
    try {
      // æ›´æ–°ä»»åŠ¡çŠ¶æ€
      task.status = "downloading"
      task.startedAt = new Date()
      this.emit("download:started", task)

      // æ›´æ–°æ¨¡å‹çŠ¶æ€
      const model = this.models.get(task.modelId)
      if (model) {
        model.status = "downloading"
        this.models.set(task.modelId, model)
        this.emit("model:updated", model)
      }

      console.log(`ğŸ”„ å¼€å§‹ä¸‹è½½æ¨¡å‹: ${task.modelId}`)

      // è°ƒç”¨Ollama APIä¸‹è½½æ¨¡å‹
      const response = await this.makeRequest(
        "/api/pull",
        "POST",
        { name: task.modelId },
        300000, // 5åˆ†é’Ÿè¶…æ—¶
      )

      // å¤„ç†æµå¼å“åº”
      await this.handleDownloadStream(response, task)

      // ä¸‹è½½å®Œæˆ
      task.status = "completed"
      task.progress = 100
      task.completedAt = new Date()
      this.emit("download:completed", task)

      // æ›´æ–°æ¨¡å‹çŠ¶æ€
      if (model) {
        model.status = "ready"
        this.models.set(task.modelId, model)
        this.emit("model:updated", model)
      }

      console.log(`âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ: ${task.modelId}`)

      // é‡æ–°åŠ è½½æ¨¡å‹ä¿¡æ¯
      await this.loadModels()
    } catch (error) {
      console.error(`âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥: ${task.modelId}`, error)

      // æ›´æ–°ä»»åŠ¡çŠ¶æ€
      task.status = "failed"
      task.error = error instanceof Error ? error.message : "ä¸‹è½½å¤±è´¥"
      task.updatedAt = new Date()
      this.emit("download:failed", task)

      // æ›´æ–°æ¨¡å‹çŠ¶æ€
      const model = this.models.get(task.modelId)
      if (model) {
        model.status = "download_failed"
        this.models.set(task.modelId, model)
        this.emit("model:updated", model)
      }
    } finally {
      // ä»æ´»åŠ¨ä¸‹è½½ä¸­ç§»é™¤
      this.activeDownloads.delete(task.modelId)

      // ç»§ç»­å¤„ç†é˜Ÿåˆ—
      this.processDownloadQueue()
    }
  }

  // å¤„ç†ä¸‹è½½æµ
  private async handleDownloadStream(response: Response, task: DownloadTask): Promise<void> {
    const reader = response.body?.getReader()
    if (!reader) {
      throw new Error("æ— æ³•è·å–å“åº”æµ")
    }

    const decoder = new TextDecoder()
    let totalBytes = 0
    let downloadedBytes = 0
    let lastUpdateTime = Date.now()
    let lastDownloadedBytes = 0

    try {
      while (true) {
        const { done, value } = await reader.read()
        if (done) break

        const chunk = decoder.decode(value, { stream: true })
        const lines = chunk.split("\n").filter((line) => line.trim())

        for (const line of lines) {
          try {
            const data = JSON.parse(line)

            if (data.total && data.completed !== undefined) {
              totalBytes = data.total
              downloadedBytes = data.completed

              // è®¡ç®—è¿›åº¦
              const progress = Math.round((downloadedBytes / totalBytes) * 100)

              // è®¡ç®—ä¸‹è½½é€Ÿåº¦
              const currentTime = Date.now()
              const timeDiff = (currentTime - lastUpdateTime) / 1000 // ç§’
              const bytesDiff = downloadedBytes - lastDownloadedBytes

              if (timeDiff > 0) {
                const speed = bytesDiff / timeDiff // å­—èŠ‚/ç§’
                const remainingBytes = totalBytes - downloadedBytes
                const eta = remainingBytes > 0 ? Math.round(remainingBytes / speed) : 0

                // æ›´æ–°ä»»åŠ¡ä¿¡æ¯
                task.progress = progress
                task.speed = speed
                task.eta = eta
                task.updatedAt = new Date()

                // å‘é€è¿›åº¦æ›´æ–°äº‹ä»¶
                this.emit("download:progress", {
                  taskId: task.id,
                  modelId: task.modelId,
                  progress,
                  speed,
                  eta,
                  downloadedBytes,
                  totalBytes,
                })

                // æ›´æ–°æ—¶é—´å’Œå­—èŠ‚æ•°
                lastUpdateTime = currentTime
                lastDownloadedBytes = downloadedBytes
              }
            }

            if (data.status) {
              console.log(`ğŸ“¥ ${task.modelId}: ${data.status}`)
            }
          } catch (parseError) {
            // å¿½ç•¥JSONè§£æé”™è¯¯
          }
        }
      }
    } finally {
      reader.releaseLock()
    }
  }

  // åˆ é™¤æ¨¡å‹
  public async deleteModel(modelId: string): Promise<boolean> {
    try {
      const model = this.models.get(modelId)
      if (!model) {
        throw new Error(`æ¨¡å‹ä¸å­˜åœ¨: ${modelId}`)
      }

      if (model.status === "downloading") {
        throw new Error(`æ¨¡å‹æ­£åœ¨ä¸‹è½½ä¸­ï¼Œæ— æ³•åˆ é™¤: ${modelId}`)
      }

      console.log(`ğŸ—‘ï¸ å¼€å§‹åˆ é™¤æ¨¡å‹: ${modelId}`)

      await this.makeRequest("/api/delete", "DELETE", { name: modelId })

      // æ›´æ–°æ¨¡å‹çŠ¶æ€
      model.status = "not_downloaded"
      model.size = 0
      this.models.set(modelId, model)
      this.emit("model:updated", model)

      console.log(`âœ… æ¨¡å‹åˆ é™¤æˆåŠŸ: ${modelId}`)
      return true
    } catch (error) {
      console.error(`âŒ æ¨¡å‹åˆ é™¤å¤±è´¥: ${modelId}`, error)
      return false
    }
  }

  // ç”Ÿæˆæ–‡æœ¬ï¼ˆå¸¦æ€§èƒ½ç›‘æ§ï¼‰
  public async generateText(modelId: string, prompt: string, options: GenerateOptions = {}): Promise<GenerateResponse> {
    const startTime = Date.now()
    const model = this.models.get(modelId)

    if (!model) {
      throw new Error(`æ¨¡å‹ä¸å­˜åœ¨: ${modelId}`)
    }

    if (model.status !== "ready") {
      throw new Error(`æ¨¡å‹æœªå°±ç»ª: ${modelId}`)
    }

    try {
      // æ ‡è®°æ¨¡å‹ä¸ºä½¿ç”¨ä¸­
      model.status = "busy"
      this.models.set(modelId, model)
      this.emit("model:updated", model)

      const response = await this.makeRequest("/api/generate", "POST", {
        model: modelId,
        prompt,
        stream: false,
        options: {
          temperature: options.temperature || 0.7,
          top_p: options.topP || 0.9,
          top_k: options.topK || 40,
          num_predict: options.maxTokens || 2048,
          ...options.rawOptions,
        },
      })

      const result = await response.json()
      const endTime = Date.now()
      const latency = endTime - startTime

      // æ›´æ–°ä½¿ç”¨ç»Ÿè®¡
      model.usageStats.totalCalls++
      model.usageStats.totalTokens += result.eval_count || 0
      model.usageStats.averageLatency =
        (model.usageStats.averageLatency * (model.usageStats.totalCalls - 1) + latency) / model.usageStats.totalCalls
      model.usageStats.lastUsed = new Date()

      // æ›´æ–°æ€§èƒ½æŒ‡æ ‡
      if (result.eval_count && result.eval_duration) {
        model.performance.tokensPerSecond = (result.eval_count / result.eval_duration) * 1e9
      }

      // æ¢å¤æ¨¡å‹çŠ¶æ€
      model.status = "ready"
      this.models.set(modelId, model)
      this.emit("model:updated", model)

      return {
        success: true,
        text: result.response,
        model: modelId,
        tokens: {
          prompt: result.prompt_eval_count || 0,
          completion: result.eval_count || 0,
          total: (result.prompt_eval_count || 0) + (result.eval_count || 0),
        },
        timing: {
          promptEvalTime: result.prompt_eval_duration || 0,
          evalTime: result.eval_duration || 0,
          totalTime: result.total_duration || 0,
        },
        latency,
        metadata: result,
      }
    } catch (error) {
      // æ›´æ–°é”™è¯¯ç»Ÿè®¡
      if (model) {
        model.usageStats.errorCount++
        model.status = "ready"
        this.models.set(modelId, model)
        this.emit("model:updated", model)
      }

      return {
        success: false,
        error: error instanceof Error ? error.message : "ç”Ÿæˆå¤±è´¥",
        model: modelId,
        latency: Date.now() - startTime,
      }
    }
  }

  // è·å–æ¨¡å‹æ¨è
  public getRecommendedModels(
    type?: ModelType,
    maxCount = 5,
    criteria: "performance" | "usage" | "size" = "usage",
  ): EnhancedOllamaModel[] {
    let models = Array.from(this.models.values()).filter((model) => model.status === "ready")

    if (type) {
      models = models.filter((model) => model.type === type)
    }

    // æ ¹æ®æ¨èæ ‡å‡†æ’åº
    switch (criteria) {
      case "performance":
        models.sort((a, b) => {
          const aScore = a.performance.tokensPerSecond - a.usageStats.averageLatency / 1000
          const bScore = b.performance.tokensPerSecond - b.usageStats.averageLatency / 1000
          return bScore - aScore
        })
        break

      case "usage":
        models.sort((a, b) => {
          const aScore = a.usageStats.totalCalls * 0.7 + (a.usageStats.lastUsed ? 1 : 0) * 0.3
          const bScore = b.usageStats.totalCalls * 0.7 + (b.usageStats.lastUsed ? 1 : 0) * 0.3
          return bScore - aScore
        })
        break

      case "size":
        models.sort((a, b) => a.size - b.size)
        break
    }

    return models.slice(0, maxCount)
  }

  // è·å–æ¨¡å‹ç»Ÿè®¡
  public getModelStatistics(): ModelStatistics {
    const models = Array.from(this.models.values())

    return {
      total: models.length,
      ready: models.filter((m) => m.status === "ready").length,
      downloading: models.filter((m) => m.status === "downloading").length,
      queued: models.filter((m) => m.status === "queued").length,
      notDownloaded: models.filter((m) => m.status === "not_downloaded").length,
      failed: models.filter((m) => m.status === "download_failed").length,
      byType: {
        chat: models.filter((m) => m.type === "chat").length,
        code: models.filter((m) => m.type === "code").length,
        multimodal: models.filter((m) => m.type === "multimodal").length,
      },
      totalSize: models.reduce((sum, m) => sum + m.size, 0),
      totalCalls: models.reduce((sum, m) => sum + m.usageStats.totalCalls, 0),
      totalTokens: models.reduce((sum, m) => sum + m.usageStats.totalTokens, 0),
      averageLatency:
        models.length > 0 ? models.reduce((sum, m) => sum + m.usageStats.averageLatency, 0) / models.length : 0,
    }
  }

  // å¥åº·æ£€æŸ¥
  private startHealthCheck(): void {
    this.healthCheckInterval = setInterval(async () => {
      try {
        await this.checkConnection()
      } catch (error) {
        console.warn("å¥åº·æ£€æŸ¥å¤±è´¥:", error)
        this.scheduleReconnect()
      }
    }, 30000) // æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡
  }

  // é‡è¿è°ƒåº¦
  private scheduleReconnect(): void {
    if (this.reconnectAttempts >= this.maxReconnectAttempts) {
      console.error("è¾¾åˆ°æœ€å¤§é‡è¿æ¬¡æ•°ï¼Œåœæ­¢é‡è¿")
      this.emit("connection:failed")
      return
    }

    this.connectionStatus = "reconnecting"
    this.reconnectAttempts++

    const delay = Math.min(1000 * Math.pow(2, this.reconnectAttempts), 30000) // æŒ‡æ•°é€€é¿ï¼Œæœ€å¤§30ç§’

    console.log(`ğŸ”„ ${delay / 1000}ç§’åå°è¯•é‡è¿ (${this.reconnectAttempts}/${this.maxReconnectAttempts})`)

    setTimeout(async () => {
      try {
        await this.checkConnection()
        await this.loadModels()
        console.log("âœ… é‡è¿æˆåŠŸ")
      } catch (error) {
        console.error("é‡è¿å¤±è´¥:", error)
        this.scheduleReconnect()
      }
    }, delay)
  }

  // å·¥å…·æ–¹æ³•
  private formatModelName(modelId: string): string {
    const parts = modelId.split(":")
    const baseName = parts[0]
    const version = parts[1] || ""

    const nameMap: Record<string, string> = {
      codellama: "CodeLlama",
      llama3: "Llama 3",
      llama2: "Llama 2",
      phi3: "Phi-3",
      mistral: "Mistral",
      qwen: "Qwen",
      qwen2: "Qwen2",
      gemma: "Gemma",
    }

    const formattedName = nameMap[baseName] || baseName.charAt(0).toUpperCase() + baseName.slice(1)
    return version ? `${formattedName} ${version}` : formattedName
  }

  private inferModelType(modelId: string): ModelType {
    const id = modelId.toLowerCase()
    if (id.includes("code")) return "code"
    if (id.includes("vision") || id.includes("multimodal")) return "multimodal"
    return "chat"
  }

  private inferModelParameters(modelId: string): string {
    const id = modelId.toLowerCase()
    if (id.includes("70b")) return "70B"
    if (id.includes("34b")) return "34B"
    if (id.includes("13b")) return "13B"
    if (id.includes("8b")) return "8B"
    if (id.includes("7b")) return "7B"
    if (id.includes("3b")) return "3B"
    if (id.includes("1b")) return "1B"
    return "æœªçŸ¥"
  }

  private inferModelQuantization(modelId: string): string {
    const id = modelId.toLowerCase()
    if (id.includes("q4_0")) return "Q4_0"
    if (id.includes("q4_1")) return "Q4_1"
    if (id.includes("q5_0")) return "Q5_0"
    if (id.includes("q5_1")) return "Q5_1"
    if (id.includes("q8_0")) return "Q8_0"
    return "æ— é‡åŒ–"
  }

  // è·å–æ‰€æœ‰æ¨¡å‹
  public getAllModels(): EnhancedOllamaModel[] {
    return Array.from(this.models.values())
  }

  // è·å–è¿æ¥çŠ¶æ€
  public getConnectionStatus(): "connected" | "disconnected" | "reconnecting" {
    return this.connectionStatus
  }

  // è·å–æ´»åŠ¨ä¸‹è½½ä»»åŠ¡
  public getActiveDownloads(): DownloadTask[] {
    return Array.from(this.activeDownloads.values())
  }

  // è·å–ä¸‹è½½é˜Ÿåˆ—
  public getDownloadQueue(): DownloadTask[] {
    return [...this.downloadQueue]
  }

  // æ¸…ç†èµ„æº
  public destroy(): void {
    if (this.healthCheckInterval) {
      clearInterval(this.healthCheckInterval)
      this.healthCheckInterval = null
    }
    this.removeAllListeners()
  }
}

// ç±»å‹å®šä¹‰
export type ModelType = "chat" | "code" | "multimodal"
export type ModelStatus = "ready" | "busy" | "downloading" | "queued" | "not_downloaded" | "download_failed"
export type TaskStatus = "queued" | "downloading" | "completed" | "failed"

export interface EnhancedOllamaModel {
  id: string
  name: string
  type: ModelType
  provider: "ollama"
  status: ModelStatus
  size: number
  digest: string
  modifiedAt: Date
  parameters: string
  quantization: string
  usageStats: {
    totalCalls: number
    totalTokens: number
    averageLatency: number
    lastUsed: Date | null
    errorCount: number
  }
  performance: {
    tokensPerSecond: number
    memoryUsage: number
    cpuUsage: number
  }
  createdAt: Date
}

export interface DownloadTask {
  id: string
  modelId: string
  type: "download"
  status: TaskStatus
  progress: number
  speed: number
  eta: number | null
  createdAt: Date
  updatedAt: Date
  startedAt?: Date
  completedAt?: Date
  error?: string
}

export interface GenerateOptions {
  temperature?: number
  topP?: number
  topK?: number
  maxTokens?: number
  rawOptions?: Record<string, any>
}

export interface GenerateResponse {
  success: boolean
  text?: string
  error?: string
  model: string
  tokens?: {
    prompt: number
    completion: number
    total: number
  }
  timing?: {
    promptEvalTime: number
    evalTime: number
    totalTime: number
  }
  latency?: number
  metadata?: any
}

export interface ModelStatistics {
  total: number
  ready: number
  downloading: number
  queued: number
  notDownloaded: number
  failed: number
  byType: Record<ModelType, number>
  totalSize: number
  totalCalls: number
  totalTokens: number
  averageLatency: number
}

// å¯¼å‡ºå¢å¼ºç‰ˆOllamaæœåŠ¡å®ä¾‹
export const enhancedOllamaService = EnhancedOllamaService.getInstance()
